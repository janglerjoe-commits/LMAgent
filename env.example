# Copy this file to .env and fill in your values.
# .env is gitignored — never commit it.

# Directory the agent reads/writes files in (absolute path recommended)
WORKSPACE="/home/you/lmagent_workspace"

# LLM API endpoint — default works with LM Studio out of the box
LLM_URL="http://localhost:1234/v1/chat/completions"

# API key — any string works for local servers
LLM_API_KEY="lm-studio"

# Model name to request — leave blank to use whatever is loaded in LM Studio
LLM_MODEL=""

# Permission mode: auto (no prompts) | normal (prompts on destructive actions) | manual (always prompt)
PERMISSION_MODE="normal"

# --- Optional tuning ---

# Hard cap on iterations per task
# MAX_ITERATIONS=150

# LLM sampling temperature
# TEMPERATURE=0.9

# Seconds to wait for an LLM response before timing out
# LLM_TIMEOUT=560

# Strip <think> blocks from model output (set true for QwQ, DeepSeek-R1, etc.)
# THINKING_MODEL=true

# Token count at which the context window gets compacted
# SUMMARIZATION_THRESHOLD=80000

# Enable/disable features
# ENABLE_SUB_AGENTS=true
# ENABLE_TODO_TRACKING=true
# ENABLE_PLAN_ENFORCEMENT=true

# Seconds between scheduler wake-up checks
# SCHEDULER_POLL_INTERVAL=60
